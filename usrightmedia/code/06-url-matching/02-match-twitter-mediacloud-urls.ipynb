{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL matching\n",
    "\n",
    "**Purpose**: check each outlet's document to see if it was (re-)tweeted, and if so, how many times.\n",
    "\n",
    "1. Update `standardized_url` field to use the updated version of urlExpander\n",
    "2. Add matches: for each outlet's document, check if `standardized_url` matches 1+ documents of `tweets2_url` doctype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib is logged even though disable_existing_loggers=yes in logging_config.yaml\n",
    "# https://stackoverflow.com/a/51529172/7016397\n",
    "# workaround is to manually set the level before creating my logger\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from usrightmedia.shared.loggers import get_logger\n",
    "LOGGER = get_logger(filename = '01-url-matching', logger_type='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-processing uses the Bulk API through the Python Elasticsearch client.\n",
    "- Elasticsearch: [\"How Big Is Too Big?\"](https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html#_how_big_is_too_big)\n",
    ">The entire bulk request needs to be loaded into memory by the node that receives our request, so the bigger the request, the less memory available for other requests. There is an optimal size of bulk request. Above that size, performance no longer improves and may even drop off. The optimal size, however, is not a fixed number. It depends entirely on your hardware, your document size and complexity, and your indexing and search load.\n",
    ">Fortunately, it is easy to find this sweet spot: Try indexing typical documents in batches of increasing size. When performance starts to drop off, your batch size is too big. A good place to start is with batches of 1,000 to 5,000 documents or, if your documents are very large, with even smaller batches.\n",
    ">It is often useful to keep an eye on the physical size of your bulk requests. One thousand 1KB documents is very different from one thousand 1MB documents. A good bulk size to start playing with is around 5-15MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulksize = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import INCA and check doctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-10 10:53:11,951 - [WARNING] - INCA - (hype_analysis.py).<module>(21) - $DISPLAY environment variable is not set, trying a different approach. You probably are running INCA on a text console, right?\n",
      "2021-12-10 10:53:12,206 - [WARNING] - INCA - (var_tsa_analysis.py).<module>(21) - $DISPLAY environment variable is not set, trying a different approach. You probably are running INCA on a text console, right?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tweets2': 889739,\n",
       " 'tweets2_url': 285447,\n",
       " 'foxnews': 63571,\n",
       " 'breitbart': 39412,\n",
       " 'dailycaller': 29463,\n",
       " 'oneamericanews': 23008,\n",
       " 'washingtonexaminer': 20793,\n",
       " 'newsmax': 12345,\n",
       " 'gatewaypundit': 9508,\n",
       " 'infowars': 6751,\n",
       " 'vdare': 6545,\n",
       " 'dailystormer': 4005,\n",
       " 'rushlimbaugh': 2533,\n",
       " 'americanrenaissance': 2284,\n",
       " 'seanhannity': 1232}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inca import Inca\n",
    "myinca = Inca()\n",
    "myinca.database.list_doctypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Add `standardized_url` field to documents representing (re-)tweeted URLs and outlets' articles\n",
    "- `force=True` to use updated version of urlExpander (public fork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctypes_to_process = [\n",
    "    \"americanrenaissance\",\n",
    "    \"breitbart\",\n",
    "    \"dailycaller\",\n",
    "    \"dailystormer\",\n",
    "    \"foxnews\",\n",
    "    \"gatewaypundit\",\n",
    "    \"infowars\",\n",
    "    \"newsmax\",\n",
    "    \"oneamericanews\",\n",
    "    \"rushlimbaugh\",\n",
    "    \"seanhannity\",\n",
    "    \"vdare\",\n",
    "    \"washingtonexaminer\",\n",
    "    \"tweets2_url\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2284/2284 [00:41<00:00, 55.62it/s]  \n",
      "100%|██████████| 39412/39412 [10:16<00:00, 63.97it/s] \n",
      "100%|██████████| 29463/29463 [07:00<00:00, 70.11it/s] \n",
      "100%|██████████| 4005/4005 [00:53<00:00, 74.57it/s]  \n",
      "100%|██████████| 63571/63571 [18:43<00:00, 56.57it/s] \n",
      "100%|██████████| 9508/9508 [01:46<00:00, 89.00it/s]  \n",
      "100%|██████████| 6751/6751 [01:19<00:00, 84.40it/s]  \n",
      "100%|██████████| 12345/12345 [02:55<00:00, 70.17it/s] \n",
      "100%|██████████| 23008/23008 [05:28<00:00, 70.14it/s] \n",
      "100%|██████████| 2533/2533 [00:45<00:00, 55.07it/s]  \n",
      "100%|██████████| 1232/1232 [00:00<00:00, 4569.37it/s]\n",
      "100%|██████████| 6545/6545 [01:56<00:00, 56.04it/s]  \n",
      "100%|██████████| 20793/20793 [05:06<00:00, 67.90it/s] \n",
      "100%|██████████| 285447/285447 [33:39<00:00, 141.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for doctype in doctypes_to_process:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        docs = myinca.processing.standardize_url(docs_or_query=doctype,\n",
    "                                                 field=\"resolved_url\",\n",
    "                                                 save=True,\n",
    "                                                 new_key=\"standardized_url\",\n",
    "                                                 action=\"batch\",\n",
    "                                                 bulksize=bulksize,\n",
    "                                                 force=True)\n",
    "        for doc in docs:\n",
    "            # runs process on doc\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find matches\n",
    "- For each outlet's document, check if `standardized_url` matches any documents of the `tweets2_url` doctype.\n",
    "- Store the results in the new keys: `tweets2_url_ids` (list of matched IDs), `tweets2_url_count` (number of matches), and `tweets2_url_ind` (boolean indicator of 1+ matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_doctypes = [\n",
    "    \"americanrenaissance\",\n",
    "    \"breitbart\",\n",
    "    \"dailycaller\",\n",
    "    \"dailystormer\",\n",
    "    \"foxnews\",\n",
    "    \"gatewaypundit\",\n",
    "    \"infowars\",\n",
    "    \"newsmax\",\n",
    "    \"oneamericanews\",\n",
    "    \"rushlimbaugh\",\n",
    "    \"seanhannity\",\n",
    "    \"vdare\",\n",
    "    \"washingtonexaminer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2284/2284 [00:46<00:00, 49.15it/s]\n",
      "100%|██████████| 39412/39412 [12:09<00:00, 54.03it/s]\n",
      "100%|██████████| 29463/29463 [08:43<00:00, 56.29it/s]\n",
      "100%|██████████| 4005/4005 [01:06<00:00, 60.14it/s]\n",
      "100%|██████████| 63571/63571 [18:54<00:00, 56.02it/s]\n",
      "100%|██████████| 9508/9508 [02:25<00:00, 65.41it/s] \n",
      "100%|██████████| 6751/6751 [01:46<00:00, 63.27it/s]\n",
      "100%|██████████| 12345/12345 [03:19<00:00, 61.75it/s]\n",
      "100%|██████████| 23008/23008 [06:17<00:00, 60.96it/s] \n",
      "100%|██████████| 2533/2533 [00:54<00:00, 46.69it/s]\n",
      "100%|██████████| 1232/1232 [00:18<00:00, 67.24it/s]\n",
      "100%|██████████| 6545/6545 [02:06<00:00, 51.72it/s]\n",
      "100%|██████████| 20793/20793 [05:36<00:00, 61.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# action=\"run\" rather than action=\"batch\" because this processor sends an individual HTTP request to Elasticsearch per document anyway\n",
    "for doctype in outlet_doctypes:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        docs = myinca.processing.match_outlet_articles_to_tweets2_urls(docs_or_query=doctype,\n",
    "                                                                       field=\"standardized_url\",\n",
    "                                                                       save=True,\n",
    "                                                                       new_key=\"tweets2_url_ids\",\n",
    "                                                                       action=\"run\",\n",
    "                                                                       force=True)\n",
    "        for doc in docs:\n",
    "            # runs process on doc\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2284/2284 [00:38<00:00, 59.09it/s]  \n",
      "100%|██████████| 39412/39412 [10:45<00:00, 61.10it/s] \n",
      "100%|██████████| 29463/29463 [07:17<00:00, 67.32it/s] \n",
      "100%|██████████| 4005/4005 [00:54<00:00, 73.69it/s]  \n",
      "100%|██████████| 63571/63571 [18:51<00:00, 56.16it/s] \n",
      "100%|██████████| 9508/9508 [01:55<00:00, 82.26it/s]  \n",
      "100%|██████████| 6751/6751 [01:32<00:00, 72.68it/s]  \n",
      "100%|██████████| 12345/12345 [03:06<00:00, 66.23it/s] \n",
      "100%|██████████| 23008/23008 [05:50<00:00, 65.68it/s] \n",
      "100%|██████████| 2533/2533 [00:47<00:00, 53.68it/s]  \n",
      "100%|██████████| 1232/1232 [00:00<00:00, 4053.64it/s]\n",
      "100%|██████████| 6545/6545 [02:15<00:00, 48.14it/s]  \n",
      "100%|██████████| 20793/20793 [05:13<00:00, 66.33it/s] \n"
     ]
    }
   ],
   "source": [
    "for doctype in outlet_doctypes:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        docs = myinca.processing.match_outlet_articles_to_tweets2_urls_count(docs_or_query=doctype,\n",
    "                                                                             field=\"tweets2_url_ids\",\n",
    "                                                                             save=True,\n",
    "                                                                             new_key=\"tweets2_url_match_count\",\n",
    "                                                                             action=\"batch\",\n",
    "                                                                             bulksize=bulksize,\n",
    "                                                                             force=True)\n",
    "        for doc in docs:\n",
    "            # runs process on doc\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2284/2284 [00:41<00:00, 55.05it/s]  \n",
      "100%|██████████| 39412/39412 [10:27<00:00, 62.77it/s] \n",
      "100%|██████████| 29463/29463 [07:21<00:00, 66.66it/s] \n",
      "100%|██████████| 4005/4005 [00:57<00:00, 70.14it/s]  \n",
      "100%|██████████| 63571/63571 [18:59<00:00, 55.78it/s]\n",
      "100%|██████████| 9508/9508 [01:52<00:00, 84.30it/s]  \n",
      "100%|██████████| 6751/6751 [01:33<00:00, 72.48it/s]  \n",
      "100%|██████████| 12345/12345 [03:09<00:00, 65.31it/s] \n",
      "100%|██████████| 23008/23008 [05:53<00:00, 65.11it/s] \n",
      "100%|██████████| 2533/2533 [00:51<00:00, 49.41it/s]  \n",
      "100%|██████████| 1232/1232 [00:00<00:00, 4596.51it/s]\n",
      "100%|██████████| 6545/6545 [02:04<00:00, 52.38it/s]  \n",
      "100%|██████████| 20793/20793 [05:03<00:00, 68.44it/s] \n"
     ]
    }
   ],
   "source": [
    "for doctype in outlet_doctypes:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        docs = myinca.processing.match_outlet_articles_to_tweets2_urls_ind(docs_or_query=doctype,\n",
    "                                                                           field=\"tweets2_url_ids\",\n",
    "                                                                           save=True,\n",
    "                                                                           new_key=\"tweets2_url_match_ind\",\n",
    "                                                                           action=\"batch\",\n",
    "                                                                           bulksize=bulksize,\n",
    "                                                                           force=True)\n",
    "        for doc in docs:\n",
    "            # runs process on doc\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(e)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd43d351af792a8a2d3bed576ed992c22af60aaa9b7bcbf49f8a265b549672e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (usrightmedia)",
   "language": "python",
   "name": "usrightmedia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
