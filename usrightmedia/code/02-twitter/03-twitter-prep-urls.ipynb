{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-twitter-prep-urls\n",
    "**Purpose**:  extract URLs from tweets\n",
    "- filter out duplicates and URLs internal to Twitter since we are only interested in URLs shared external to Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from inca import Inca\n",
    "\n",
    "dir_inp = os.path.join('..', '..', 'data', '02-intermediate', '01-congress-legislators')\n",
    "dir_out = os.path.join('..', '..', 'data', '02-intermediate', '02-twitter')\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from usrightmedia.shared.es_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinca = Inca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinca.database.doctype_inspect('tweets2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Understanding the new tweet payload]( https://blog.twitter.com/developer/en_us/topics/tips/2020/understanding-the-new-tweet-payload)\n",
    "```\n",
    "If a Tweet contains a URL, you can request information about it in the new v2 Tweet payload\n",
    "using ‘entities’ as a value for the tweet.fields parameter. This will provide a ‘urls’ object\n",
    "on your Tweet object (nested in the ‘data’ object). For a URL in the Tweet, you will be able\n",
    "to get the URL, the title, description, and unwounded_url as shown below: (see link)\n",
    "```\n",
    "\n",
    "- [API Reference](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets with at least one URL\n",
    "tweets = []\n",
    "for n, doc in enumerate(myinca.database.document_generator(query_tw_field_exists('entities.urls'))):\n",
    "    tweets.append(doc['_source'])\n",
    "    # if n > 1000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(tweet):\n",
    "    \"\"\" Extract the URL(s) included within each tweet\n",
    "    \n",
    "    Args:\n",
    "        tweet (Twitter API v2 payload)\n",
    "\n",
    "    Returns:\n",
    "        extracted URLs (list of dictionaries):\n",
    "            - the tweet-level info is duplicated across every URL dict\n",
    "            - at the URL level:\n",
    "                - 'url_id' (tweet_id + URL index value within tweet)\n",
    "                - 'most_unrolled_url' is the URL for further processing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # tweet-level info\n",
    "    tweet_id =  tweet['id']\n",
    "    created_at =  datetime.datetime.strptime(tweet['created_at'],'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    text = tweet['text']\n",
    "    author_id = tweet['author_id']\n",
    "    username = tweet['author']['username']\n",
    "\n",
    "    # list of URLs within tweet\n",
    "    tweet_urls = tweet['entities']['urls']\n",
    "\n",
    "    extracted_urls = []\n",
    "\n",
    "    for n, t in enumerate(tweet_urls):\n",
    "        r = defaultdict(None)\n",
    "\n",
    "        # for every URL, include info about the tweet it came from\n",
    "        r['tweet_id'] = tweet_id\n",
    "        r['created_at'] = created_at\n",
    "        r['text'] = text\n",
    "        r['author_id'] = author_id\n",
    "        r['username'] = username\n",
    "        r['tweet_url'] = f\"https://twitter.com/{username}/status/{tweet_id}\"\n",
    "\n",
    "        # add any versions of URL which are available\n",
    "        r['url_id'] = f\"{tweet_id}_{n}\"\n",
    "        r['url'] = t.get('url', None)\n",
    "        r['expanded_url'] = t.get('expanded_url', None)\n",
    "        r['display_url'] = t.get('display_url', None)\n",
    "        r['unwound_url'] = t.get('unwound_url', None)\n",
    "        \n",
    "        # preferred URL version\n",
    "        # similar to https://github.com/twitterdev/tweet_parser/blob/master/tweet_parser/getter_methods/tweet_links.py\n",
    "        if r['unwound_url']:\n",
    "            r['most_unrolled_url'] = r['unwound_url']\n",
    "            r['most_unrolled_field'] = 'unwound_url'\n",
    "        elif r['expanded_url']:\n",
    "            r['most_unrolled_url'] = r['expanded_url']\n",
    "            r['most_unrolled_field'] = 'expanded_url'\n",
    "        elif r['url']:\n",
    "            r['most_unrolled_url'] = r['url']\n",
    "            r['most_unrolled_field'] = 'url'\n",
    "\n",
    "        extracted_urls.append(r)\n",
    "\n",
    "    return extracted_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for tweet in tweets:\n",
    "    urls.extend(extract_urls(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['url', 'expanded_url', 'display_url', 'unwound_url', 'most_unrolled_url']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of tweets: {df['tweet_id'].nunique()}\") # same as len(tweets)\n",
    "print(f\"number of URLs extracted from tweets: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of URLs each tweet object contains\n",
    "# \"60,827 tweets contain 2 URLs each\"\n",
    "vc_smry = df.groupby('tweet_id').size().value_counts()\n",
    "vc_smry = pd.DataFrame(vc_smry).reset_index().rename(columns={'index':'count (# of URLs)', 0: 'count (tweet_id)'})\n",
    "vc_smry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for filtering\n",
    "# https://stackoverflow.com/a/48628442\n",
    "vc_helper = df['tweet_id'].value_counts()\n",
    "vc_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of tweet_ids which each had 2 URLs associated with them\n",
    "df[df['tweet_id'].isin(vc_helper.index[vc_helper.eq(2)])]['tweet_id'].unique()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of tweet_ids which each had 5 URLs associated with them\n",
    "df[df['tweet_id'].isin(vc_helper.index[vc_helper.eq(5)])]['tweet_id'].unique()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tweet(tweet_id):\n",
    "    \"\"\" prints a tweet's URL, text, and info about URLs embedded in the text\n",
    "\n",
    "    Args:\n",
    "        tweet_id (str)\n",
    "    \n",
    "    Returns:\n",
    "        None    \n",
    "    \n",
    "    \"\"\"\n",
    "    t = [tweet for tweet in tweets if tweet['id']==tweet_id][0]\n",
    "    print(f\"https://twitter.com/{t['author']['username']}/status/{t['id']}\")\n",
    "    print(t['text'])\n",
    "    pprint(t['entities']['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two URLs are unique: one is internal to twitter.com and the other is not\n",
    "show_tweet('686942249113022468')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latter 4 URLs are a set of photos associated with the tweet. They share the same t.co URL and all show '...photo/1'\n",
    "# should be '...photo/1', '...photo/2', etc.\n",
    "show_tweet('1040678926274715648')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: rows where the tweet object contained duplicate URLs\n",
    "df.loc[df.duplicated(['tweet_id', 'most_unrolled_url'], keep='first')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean filter conditions\n",
    "df['is_dupe'] = df.duplicated(['tweet_id', 'most_unrolled_url'],keep='first')\n",
    "df['url_netloc'] = df['most_unrolled_url'].map(lambda x: urlparse(x).netloc)\n",
    "df['is_from_tw'] = df['url_netloc'] == 'twitter.com'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to count URLs per tweet instance and avoid 'inflating' the count due to duplicates (is_dupe==False)\n",
    "# we are also only interested in URLs shared from non-'twitter.com' (is_from_tw==False)\n",
    "filter_smry = pd.DataFrame(df.groupby(['is_dupe', 'is_from_tw'],dropna=False).size()).reset_index().rename(columns={0:'count'})\n",
    "filter_smry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a URL if it is distinct within its tweet object and not from 'twitter.com'\n",
    "df_filtered = df.loc[(df['is_dupe']==False) & (df['is_from_tw']==False)].reset_index(drop=True)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: seems like unwound_url is often unavailable from Twitter API v2\n",
    "df['most_unrolled_field'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['most_unrolled_field'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: which URLs got excluded?\n",
    "excluded_url_ids = list(set(df.loc[df['most_unrolled_field'].notnull()]['url_id']) - set(df_filtered.loc[df_filtered['most_unrolled_field'].notnull()]['url_id']))\n",
    "df.loc[df['url_id'].isin(excluded_url_ids)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_filtered)} URLs shared by politicians on Twitter which should be processed further (URL expansion and URL matching)\")\n",
    "df_filtered.to_pickle(os.path.join(dir_out, f'politicians_tweeted_urls.pkl'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd43d351af792a8a2d3bed576ed992c22af60aaa9b7bcbf49f8a265b549672e7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
